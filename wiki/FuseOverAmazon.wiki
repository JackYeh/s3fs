FUSE filesystem backed by Amazon S3

= Overview =

s3fs is a [http://fuse.sourceforge.net fuse] filesystem that allows you to mount an Amazon S3 bucket as a local filesystem. It stores files natively and transparently in S3 (i.e., you can use other programs to access the same files). Maximum file size=5G.

Its quite useful and stable, e.g., can be used to easily copy daily backup tarballs to s3.

To use it:

 # get an amazon s3 account!
 # download the source, compile it (I've used fc5/ppc and f7/i386) and slap the binary in, say, /usr/bin/s3fs
 # do this:

{{{
/usr/bin/s3fs mybucket -o accessKeyId=aaa -o secretAccessKey=bbb /mnt
}}}

That's it! the contents of your amazon bucket "mybucket" should now be accessible read/write in /mnt

If you don't like specifying your secretAccessKey on the command line
then you can create a file "/etc/passwd-s3fs" with a line containing a accessKeyId:secretAccessKey pair. Then the command line becomes simply:

{{{
/usr/bin/s3fs mybucket /mnt
}}}

You can have more than one set of credentials (i.e., credentials for more than one amazon s3 account) in /etc/passwd-s3fs in which case you'll have to specify -o accessKeyId=aaa on the command line.

Hosting a cvsroot on s3 works! Although you probably don't really want to do it in practice. E.g., cvs -d /s3/cvsroot init.

~~Using rsync with an s3 volume as the destination doesn't quite work because of timestamp issues. s3fs does not (yet) support changing timestamps on files. I mean, it will work, as in it will copy files, but, the timestamps will just be current timestamps (rsync will complain about not being able to set timestamps but will continue).~~

s3fs now works with rsync! (as of svn 43) Due to the way FUSE works and s3fs' "brute-force" support of mode (chmod) and mtime (touch), upon first sync, files are uploaded more than once, however, subsequent rsyncs are pretty much as fast as can be.

= Details =

If enabled via "use_cache" option, s3fs automatically maintains a local cache of files in the folder "~/.s3fs". Whenever s3fs needs to read or write a file on s3 it first downloads the entire file locally to .s3fs and operates on it. When fuse release() is called, s3fs will re-upload the file to s3 if it has been changed. s3fs uses md5 checksums to minimize downloads from s3.

"~/.s3fs" is just a local cache. It can be deleted at any time. s3fs will rebuild it on demand.

New for svn 43: Local file cache is disabled and I might not bring it back. I originally added local file cache thinking it would help for rsync (and createrepo). It ends up rsync works reasonably well without it. For createrepo, just rsync back and forth!

s3fs supports chmod (mode) and touch (mtime) by virtue of "x-amz-meta-mode" and "x-amz-meta-mtime" custom meta headers. As well, these are supported in a brute-force manner. That is, changing any x-amz-meta headers requires re-uploading the s3 object. This is exactly what s3fs does. When changing mode or mtime, s3fs will download the s3 object, change the meta header(s) and re-upload the s3 object. Ditto for file rename.

= Release Notes =
 * svn 66 2008-02-18
  * local file cache is back! however, it is disabled by default... use "use_cache" option, e.g., /usr/bin/s3fs mybucket /s3 -ouse_cache=1
 * svn 57 2008-02-18
  * a few bug fixes:
   * touch x-amz-meta-mtime in flush()
   * use INFILE_LARGE (libcurl) (found on fc5/ppc)
  * tidyup
 * svn 43 2008-02-17
  * mode (i.e., chmod), mtime and deep rename! rsync now works!
  * temporarily disabled local file cache (might not bring it back!)
 * svn 28 2007-12-15
  * retry on 500 server error
 * svn 27 2007-12-15
  * file-based (instead of memory-based)
   * this means that s3fs will no longer allocate large memory buffers when writing files to s3

= Faq =

 * What do I need to know?
  * /usr/bin/s3fs
  * an entry in /etc/fstab (optional)
  * the file /etc/passwd-s3fs (optional)
  * ~~ the folder ~/.s3fs (local file cache automatically maintained by s3fs) ~~
  * stores files natively and transparently in amazon s3; you can access files with other tools, e.g., jets3t
 * Why do I get "Input/output error"?
  * Does the bucket exist?
  * Are your credentials correct?
  * Is your local clock within 15 minutes of Amazon's? (RequestTimeTooSkewed)
 * How do I troubleshoot it?
  * Use the fuse -f switch, e.g., /usr/bin/s3fs -f my_bucket /mnt
 * Why do I see "Operation cannot be completed because you do not have sufficient privliges"
  * ~~you'll see this when a program you're using (e.g., tar, rsync) is trying to explicitly set the modification time of a file. s3fs currently does not support this. Contents of the file are ok, its just that the timestamp might not be what you're expecting. I'm working to fix this.~~ fixed in svn 43!
 * Q: when I mount a bucket only the current user can see it; other users cannot; how do I allow other users to see it? A: use 'allow_other'
  * /usr/bin/s3fs -o allow_other mybucket /mnt
  * or from /etc/fstab: s3fs#mybucket /mnt fuse allow_other,accessKeyId=aaa,secretAccessKey=bbb 0 0

= Limitations =
 * no permissions checking
 * ~~no chmod support: all files are 0775~~ fixed in svn 43!
 * no symlink support
 * ~~rename is "supported" by virtue of returning EXDEV from rename()~~ fixed in svn 43! svn 43 supports deep renaming of files
 * ~~ when writing files: requires as much memory as the size of the largest file you're writing (this can be easily fixed) ~~ fixed (svn 27) you should now be able to copy, say, a 2GB file to s3 without having s3fs malloc 2GB of memory!
 * deep rename directories?!?

= ToDo =

 * ~~support brute-force rename~~ fixed in svn 43
 * get symlinks working?
  * ~~ this would bog down performance: would have to do deep getattr() for every single object ~~ already doing this in svn 43... its not too bad!
 * make install target
 * get "-h" help working
 * ~~ handle utime so that rsync works! ~~ fixed in svn 43!
  * ~~ probably a bad idea after all... ~~
  * ~~ actually don't think it can be done: can't specify arbitrary create-time for PUT ~~
 * chmod support... acl
 * permissions: using -o allow_other, even though files are owned by root 0755, another use can make changes
  * use default_permissions option?!?
 * better error logging for troubleshooting, e.g., syslog...
  * need to parse response on, say, 403 and 404 errors, etc... and log 'em!
 * ~~use temporary file for flush() and then stream it to amazon~~