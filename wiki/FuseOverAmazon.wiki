#labels Featured
FUSE-based file system backed by Amazon S3

*Announcement: Please submit usage/support questions to the [http://code.google.com/p/s3fs/issues/list Issues] area instead of as a comment to this Wiki page. Thanks!*

For a commercially supported version with additional features see http://www.subcloud.com

= What's New =
 * r243 command line options accessKeyId and secretAccessKey are no longer supported
 * s3fs-1.10.tar.gz (r227) Credentials can now be supplied via environment variables, .passwd-s3fs file in the users home directory or by a file specified by a command line option.  Useful --help option. Command line error checking.
 * s3fs-1.01.tar.gz (r221) Added support for anonymously mounting public buckets
 * s3fs-1.0.tar.gz (r217) GNU-type distribution including a couple of issue fixes
 * r203 several fixes and enhancements - see release notes (below) to see all what has changed
 * r191 merged in tractis eu bucket changes http://github.com/tractis/s3fs-fork
 * r188 includes eucalyptus patch http://open.eucalyptus.com/wiki/s3fs
 * r177 fixed stale curl handle issue; fixed 100% cpu issue
 * r166 case insensitive mime type lookup
 * r152 (May 8, 2008)
  * uses x-amz-copy-source; "correct" content-type lookup via /etc/mime.types; symlinks!

= Overview =

s3fs is a [http://fuse.sourceforge.net fuse] filesystem that allows you to mount an Amazon S3 bucket as a local filesystem. It stores files natively and transparently in S3 (i.e., you can use other programs to access the same files). Maximum file size=5G.

s3fs is stable and is being used in number of production environments, e.g., rsync backup to s3.

To use it:

 # Get an Amazon S3 account! http://aws.amazon.com/s3/
 # Download, compile and install. (see InstallationNotes)
 # Specify your Security Credentials (Access Key ID & Secret Access Key) by one of the following methods:
 * using the passwd_file command line option
 * setting the AWSACCESSKEYID and AWSSECRETACCESSKEY environment variables
 * using a .passwd-s3fs file in your home directory
 * using the system-wide /etc/passwd-s3fs file
 # do this:

{{{
/usr/bin/s3fs mybucket /mnt
}}}

That's it! the contents of your amazon bucket "mybucket" should now be accessible read/write in /mnt
 

You can have more than one set of credentials. In the s3fs password file, the format is as follows:

[bucket:]accessKeyId:secretAccessKey

s3fs supports mode (e.g., chmod), mtime (e.g, touch) and uid/gid (chown). s3fs stores the values in x-amz-meta custom meta headers, ~~and as such does "brute-force" re-uploads of s3 objects if/when mode and/or mtime changes.~~ and uses x-amz-copy-source to efficiently change them.

s3fs has a caching mechanism: You can enable local file caching to minimize downloads, e.g., :
{{{
/usr/bin/s3fs mybucket /mnt -ouse_cache=/tmp
}}}

Hosting a cvsroot on s3 works! Although you probably don't really want to do it in practice. E.g., cvs -d /s3/cvsroot init. Incredibly, mysqld also works, although I doube you really wanna do that in practice! =)

~~Using rsync with an s3 volume as the destination doesn't quite work because of timestamp issues. s3fs does not (yet) support changing timestamps on files. I mean, it will work, as in it will copy files, but, the timestamps will just be current timestamps (rsync will complain about not being able to set timestamps but will continue).~~

s3fs works with rsync! (as of svn 43) ~~Due to the way FUSE works and s3fs' "brute-force" support of mode (chmod) and mtime (touch), upon first sync, files are downloaded/uploaded more than once (because rsync does (a) chmod (b) touch and (c) rename), however, subsequent rsyncs are pretty much as fast as can be. If that's too much downloading/downloading for ya then try using the "use_cache" option to enable the local file cache... it will definitely minimize the number of downloads.~~ as of r152 s3fs uses x-amz-copy-source for efficient update of mode, mtime and uid/gid.

s3fs will retry s3 transactions on certain error conditions. The default retry count is 2, i.e., s3fs will make 2 retries per s3 transaction (for a total of 3 attempts: 1st attempt + 2 retries) before giving up. You can set the retry count by using the "retries" option, e.g., "-oretries=2".

= Options =

 * *default_acl* (default="private")
  * the default canned acl to apply to all written s3 objects, e.g., "public-read"
   * see http://docs.amazonwebservices.com/AmazonS3/2006-03-01/index.html?RESTAccessPolicy.html "Canned Access Policies" for the full list of canned acls
  * any created files will have this canned acl
  * any updated files will also have this canned acl applied!

 * *prefix* (default="") (coming soon!)
  * a prefix to append to all s3 objects

 * *retries* (default="2")
  * number of times to retry a failed s3 transaction

 * *use_cache* (default="" which means disabled)
  * local folder to use for local file cache

 * *use_rrs* (default="" which means diabled)
  * use Amazon's Reduced Redundancy Storage

 * *passwd_file* (default="")
  * specify the path to the password file, over-rides looking for the password in in $HOME/.passwd-s3fs and /etc/passwd-s3fs

 * *public_bucket* (default="" which means disabled)
  * anonymously mount a public bucket when set to 1, ignores the $HOME/.passwd-s3fs and /etc/passwd-s3fs files

 * *connect_timeout* (default="2" seconds)
  * time to wait for connection before giving up

 * *readwrite_timeout* (default="10" seconds)
  * time to wait between read/write activity before giving up

 * *url* (default="http://s3.amazonaws.com")
  * sets the url to use to access amazon s3, e.g., if you want to use https then set url=https://s3.amazonaws.com

= Details =

If enabled via "use_cache" option, s3fs automatically maintains a local cache of files in the folder specified by use_cache. Whenever s3fs needs to read or write a file on s3 it first downloads the entire file locally to the folder specified by use_cache and operates on it. When fuse release() is called, s3fs will re-upload the file to s3 if it has been changed. s3fs uses md5 checksums to minimize downloads from s3.

The folder specified by use_cache is just a local cache. It can be deleted at any time. s3fs re-builds it on demand.

~~New for svn 43: Local file cache is disabled and I might not bring it back. I originally added local file cache thinking it would help for rsync (and createrepo). It ends up rsync works reasonably well without it. For createrepo, just rsync back and forth!~~

s3fs supports chmod (mode) and touch (mtime) by virtue of "x-amz-meta-mode" and "x-amz-meta-mtime" custom meta headers. ~~As well, these are supported in a brute-force manner. That is, changing any x-amz-meta headers requires re-uploading the s3 object. This is exactly what s3fs does. When changing mode or mtime, s3fs will download the s3 object, change the meta header(s) and re-upload the s3 object. Ditto for file rename.~~ as of r149 s3fs uses x-amz-copy-source, this means that s3fs no longer needs to operate in a brute-force manner; much faster now (one minor performance-related corner case left to solve... /usr/bin/touch)

Local file caching works by calculating and comparing md5 checksums (ETag HTTP header).

~~All s3 objects written by s3fs have a Content-Type of either "application/octet-stream" or "application/x-directory".~~ as of r152, s3fs now leverages /etc/mime.types to "guess" the "correct" content-type based on file name extension. This means that you can copy a website to s3 and serve it up directly from s3 with correct content-types!

= Release Notes =
 * r243
  * eliminate options for specifying credentials on the command line
 * r241
  * more error checking - if connected to the network, check for valid credentials and the bucket name is valid
 * r240
  * re-enable the the -f command line option (disables FUSE daemon mode)
 * r235
  * add debug option to send debug messages to syslog
 * r233
  * check for too liberal permissions on password file
 * r231
  * more robust error checking
 * r230
  * add "make check" target
 * r229
  * added support for multiple credentials
 * r227
  * basis for s3fs-1.10.tar.gz
 * r217
  * basis for s3fs-1.0.tar.gz
 * r213
  * added command line --version option
 * r209
  * converted to Autotools
 * r203
  * fail with error when bucket with upper case characters is attempted to be mounted
 * r202
  * fixed bug associated with url = "https://"
 * r201
  * rmdir now cannot delete a non-empty directory
 * r200
  * support for Reduced Redundancy Storage
 * r198
  * rsync without the --inplace option now creates the correct content-type metadata
 * r197
  * removed compile time warning with respect to unused function
 * r195
  * fixed argument ordering with initialization of fuse_operations data structure 
 * r166
  * case-insensitive lookup of content-type from /etc/mime.types
 * r152
  * added support for symlinks... ln -s works!
 * r151
  * use /etc/mime.types
 * r150
  * added support for uid/gid... chown works!
 * r149
  * support x-amz-copy-source... rsync much faster now!
 * r145
  * log svn version at startup via syslog /var/log/messages
 * r141
  * added "url" runtime parameter
 * r136, r138
  * connect_timeout and readwrite_timeout
 * r130
  * set uid/gid to whatever getuid()/getgid() returns
  * log some stuff to syslog (i.e., /var/log/messages)
  * fixed issue 14 (local file cache bug; fixed cp, rsync, etc...)
 * r117
  * limit max-keys=20 (workaround for libcurl's 100% cpu issue?!?)
 * r116
  * added crypto locking
 * r114
  * curl_global_init
 * r107
  * use CURLOPT_NOSIGNAL
 * r106
  * rewind on retry
 * r105
  * only send x-amz-acl and x-amz-meta headers
 * r101, r102, r103
  * fixed curl_multi_timeout bug (found on mac)
 * r99
  * added "default_acl" option
 * r92
  * parallel-ized readdir(): getting a directory listing is now a lot faster
 * r88
  * removed 10s read timeout that should not have been introduced
 * r72 2008-02-18
  * use_cache now takes path to local file cache folder, e.g., /usr/bin/s3fs mybucket /s3 -ouse_cache=/tmp
 * r66 2008-02-18
  * local file cache is back! however, it is disabled by default... use "use_cache" option, ~~e.g., /usr/bin/s3fs mybucket /s3 -ouse_cache=1~~
 * r57 2008-02-18
  * a few bug fixes:
   * touch x-amz-meta-mtime in flush()
   * use INFILE_LARGE (libcurl) (found on fc5/ppc)
  * tidyup
 * r43 2008-02-17
  * mode (i.e., chmod), mtime and deep rename! rsync now works!
  * temporarily disabled local file cache (might not bring it back!)
 * r28 2007-12-15
  * retry on 500 server error
 * r27 2007-12-15
  * file-based (instead of memory-based)
   * this means that s3fs will no longer allocate large memory buffers when writing files to s3

= Faq =

 * What do I need to know?
  * /usr/bin/s3fs
  * /var/log/messages
  * an entry in /etc/fstab (optional - * requires fuse to be fully installed * issue #115)
  * the file $HOME/.passwd-s3fs or /etc/passwd-s3fs (optional)
  * the folder specified by use_cache (optional) a local file cache automatically maintained by s3fs, enabled with "use_cache" option, e.g., -ouse_cache=/tmp
  * the file /etc/mime.types
   * map of file extensions to Content-types
   * on Fedora /etc/mime.types comes from mailcap, so, you can either (a) create this file yourself or (b) do a yum install mailcap
  * stores files natively and transparently in amazon s3; you can access files with other tools, e.g., jets3t
 * Why do I get "Input/output error"?
  * Does the bucket exist?
  * Are your credentials correct?
  * Is your local clock within 15 minutes of Amazon's? (RequestTimeTooSkewed)
 * How do I troubleshoot it?
  * tail -f /var/log/messages
  * Use the fuse -f switch, e.g., /usr/bin/s3fs -f my_bucket /mnt
 * ~~Why do I see "Operation cannot be completed because you do not have sufficient privliges"~~
  * ~~you'll see this when a program you're using (e.g., tar, rsync) is trying to explicitly set the modification time of a file. s3fs currently does not support this. Contents of the file are ok, its just that the timestamp might not be what you're expecting. I'm working to fix this.~~ fixed in svn 43!
 * Its still not working!
  * Try updating your version of libcurl: I've used 7.16 and 7.17
 * Q: when I mount a bucket only the current user can see it; other users cannot; how do I allow other users to see it? A: use 'allow_other'
  * /usr/bin/s3fs -o allow_other mybucket /mnt
  * or from /etc/fstab: s3fs#mybucket /mnt fuse allow_other 0 0
 * Q: How does the local file cache work?
  * A: It is unbounded! if you want you can use a cron job (e.g., script in /etc/cron.daily) to periodically purge "~/.s3fs"... due to the reference nature of posix file systems a periodic purge will *not* interfere with the normal operation of s3fs local file cache...!
 * ~~Q: How do I change the location of the "~/.s3fs" folder?~~
  * ~~A: you don't (for now)... use a softlink~~ -ouse_cache option *is* the path used for local file cache! e.g., /usr/bin/s3fs mybucket /s3 -ouse_cache=/tmp
 * Q: s3fs uses x-amz-meta custom meta headers... will s3fs clobber any existing x-amz-meta custom header headers?
  * A: No!
 * Q: I renamed a folder and now all of the files in that folder are gone! What the?!?
  * A: Rename it back and your files will be back. s3fs does not support deep directory rename and doesn't check for it either.

= Limitations =
 * no permissions checking
 * ~~no chmod support: all files are 0775~~ fixed in svn 43!
 * ~~no symlink support~~ added in r152
 * ~~rename is "supported" by virtue of returning EXDEV from rename()~~ fixed in svn 43! svn 43 supports deep renaming of files
 * ~~ when writing files: requires as much memory as the size of the largest file you're writing (this can be easily fixed) ~~ fixed (svn 27) you should now be able to copy, say, a 2GB file to s3 without having s3fs malloc 2GB of memory!
 * deep rename directories?!?

= ToDo =

 * ~~support brute-force rename~~ fixed in svn 43
 * ~~get symlinks working?~~ added in r152
  * ~~ this would bog down performance: would have to do deep getattr() for every single object ~~ already doing this in svn 43... its not too bad!
 * ~~ make install target ~~
 * ~~ get "-h" help working ~~
 * ~~ handle utime so that rsync works! ~~ fixed in svn 43!
  * ~~ probably a bad idea after all... ~~
  * ~~ actually don't think it can be done: can't specify arbitrary create-time for PUT ~~
 * chmod support... acl
 * permissions: using -o allow_other, even though files are owned by root 0755, another use can make changes
  * use default_permissions option?!?
 * better error logging for troubleshooting, ~~e.g., syslog...~~
  * need to parse response on, say, 403 and 404 errors, etc... and log 'em!
 * ~~use temporary file for flush() and then stream it to amazon~~

= See Also =

Here is a list of other Amazon S3 filesystems:

 * ElasticDrive
 * PersistentFS
 * https://fedorahosted.org/s3fs/
 * http://code.google.com/p/s3fs-fuse/
 * http://s3backer.googlecode.com
 * http://code.google.com/p/s3ql