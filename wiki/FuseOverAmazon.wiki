FUSE filesystem backed by Amazon S3

= Overview =

s3fs is a [http://fuse.sourceforge.net fuse] filesystem that allows you to mount an Amazon S3 bucket as a local filesystem. It stores files "natively" in S3 (i.e., you can use other programs to access the same files). Maximum file size=5G.

Its quite useful and stable, e.g., can be used to easily copy daily backup tarballs to s3.

To use it:

 # get an amazon s3 account!
 # download the source, compile it (I've used fc5/ppc and f7/i386) and slap the binary in, say, /usr/bin/s3fs
 # do this:

{{{
/usr/bin/s3fs mybucket -o accessKeyId=aaa -o secretAccessKey=bbb /mnt
}}}

That's it! the contents of your amazon bucket "mybucket" should now be accessible read/write in /mnt

If you don't like specifying your secretAccessKey on the command line
then you can create a file "/etc/passwd-s3fs" with a line containing a accessKeyId:secretAccessKey pair. Then the command line becomes simply:

{{{
/usr/bin/s3fs mybucket /mnt
}}}

You can have more than one set of credentials (i.e., credentials for more than one amazon s3 account) in /etc/passwd-s3fs in which case you'll have to specify -o accessKeyId=aaa on the command line.

Hosting a cvsroot on s3 seems to work! Although you probably don't really want to do it in practice. E.g., cvs -d /s3/cvsroot init.

Using rsync with an s3 volume as the destination doesn't quite work because of timestamp issues. s3fs does not (yet) support changing timestamps on files. I mean, it will work, as in it will copy files, but, the timestamps will just be current timestamps (rsync will complain about not being able to set timestamps but will continue).

= Details =

s3fs automatically maintains a local cache of files in the folder "~/.s3fs". Whenever s3fs needs to read or write a file on s3 it first downloads the entire file locally to .s3fs and operates on it. When fuse release() is called, s3fs will re-upload the file to s3 if it has been changed. s3fs uses md5 checksums to minimize downloads from s3.

~/.s3fs is just a local cache. It can be deleted at any time. s3fs will rebuild it on demand.

= Release Notes =
 * svn 28 2007-12-15
  * retry on 500 server error
 * svn 27 2007-12-15
  * file-based (instead of memory-based)
   * this means that s3fs will no longer allocate large memory buffers when writing files to s3

= Faq =

 * What do I need to know?
  * /usr/bin/s3fs
  * an entry in /etc/fstab (optional)
  * the file /etc/passwd-s3fs (optional)
  * stores files "natively" in amazon s3; you can access files with other tools, e.g., jets3t
 * Why do I get "Input/output error"?
  * Does the bucket exist?
  * Are your credentials correct?
  * Is your local clock within 15 minutes of Amazon's? (RequestTimeTooSkewed)
 * How do I troubleshoot it?
  * Use the fuse -f switch, e.g., /usr/bin/s3fs -f my_bucket /mnt
 * Why do I see "Operation cannot be completed because you do not have sufficient privliges"
  * you'll see this when a program you're using (e.g., tar, rsync) is trying to explicitly set the modification time of a file. s3fs currently does not support this. Contents of the file are ok, its just that the timestamp might not be what you're expecting. I'm working to fix this.

= Limitations =
 * no permissions checking
 * no chmod support: all files are 0775
 * no symlink support
 * rename is "supported" by virtue of returning EXDEV from rename()
 * ~~ when writing files: requires as much memory as the size of the largest file you're writing (this can be easily fixed) ~~

= ToDo =

 * support brute-force rename
 * get symlinks working?
  * this would bog down performance: would have to do deep getattr() for every single object
 * make install target
 * get "-h" help working
 * handle utime so that rsync works!
  * probably a bad idea after all...
  * actually don't think it can be done: can't specify arbitrary create-time for PUT
 * chmod support... acl
 * permissions: using -o allow_other, even though files are owned by root 0755, another use can make changes
  * use default_permissions option?!?
 * better error logging for troubleshooting, e.g., syslog...
  * need to parse response on, say, 403 and 404 errors, etc... and log 'em!
 * use temporary file for flush() and then stream it to amazon
 
 
 
 
 
 